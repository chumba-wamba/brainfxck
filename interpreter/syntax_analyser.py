from interpreter import op_dict, op_list
from interpreter.token import Token
from interpreter.lexer import Lexer
from typing import List, Dict


class SyntaxAnalyser:
    """
        Primarily used to solve the singular purpose 
        of checking whether the "[" operator has a 
        corresponding "]" operator.

        In case the above condition is not satisfied,
        the brainfxck code does not reach the evaluation
        stage and is prematurely stopped with an error
        message. 

            Attributes:
                token_stream : List[Token]
                    the stream of tokens generated by the lexer.

            Methods:
                analyse()
                    determines whether the code is syntactically correct or not
                    based on the token stream.
    """

    def __init__(self, token_stream: List[Token]):
        """
            Constructor method to store the token stream as an instance variable.

            Parameters:
                token_stream : List[Token]
                    the stream of tokens generated by the lexer.

            Returns:
                None
        """

        # A stack to check for the condition written
        # in the class docstring
        self.stack = []
        # A list containing the stream of tokens generated
        # by the lexer.
        self.token_stream = token_stream

    def analyse(self) -> Dict:
        """
            Method to analyse whether the syntax is accurate or not and
            to generate appropriate errors.
            Utilises a simple algorithm to determine whether "[" - "]" are
            balanced or not. 

            Parameters:
                None

            Returns:
                Dict
        """
        # If a "[" operator is encountered, then push
        # it in the stack.
        # If a "]" operators is encountered and the tos
        # has a "]" element than pop tos.
        # If a "]" operators is encountered and the tos
        # is empty then return with faulty syntax.
        # If the entire token stream has been traversed
        # and the stack is not empty then return with
        # faulty syntax otherwise move the token steam
        # to the parsing stage.

        # Traversing through every token.
        for token in self.token_stream:
            # If token is "[" then push to stack.
            if token.op_name == op_dict.get("["):
                self.stack.append(token)
            # If token is "]"
            elif token.op_name == op_dict.get("]"):
                # If stack non empty and tos "[" then pop tos.
                if len(self.stack) and self.stack[-1].op_name == op_dict.get("["):
                    self.stack = self.stack[:-1]
                else:
                    return {
                        "is_faulty": True,
                        "faulty_token_list": [token],
                    }
        # If stack non empty then return with faulty
        # syntax as true, otherwise return with faulty
        # syntax as false
        if len(self.stack):
            return {
                "is_faulty": True,
                "faulty_token_list": self.stack
            }
        return {
            "is_faulty": False,
            "faulty_token_list": []
        }
